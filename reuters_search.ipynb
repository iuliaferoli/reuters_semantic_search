{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text', 'text_type', 'topics', 'lewis_split', 'cgis_split', 'old_id', 'new_id', 'places', 'people', 'orgs', 'exchanges', 'date', 'title'],\n",
       "    num_rows: 20856\n",
       "})"
      ]
     },
     "execution_count": 218,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = load_dataset(\"reuters21578\", 'ModHayes', split=\"train\")\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The first two documents have 3291 charaters\n",
      "We can split it into 24 sentences, with an average of 135.21 characters\n",
      "Or we can split it into 20 paragraphs, with an average of 159.8 charaters\n",
      "Finally, we take 6 chunks of two paragraphs each, averaging about 508.33 characters\n"
     ]
    }
   ],
   "source": [
    "def avg_str_len(lst):\n",
    "    return np.mean([len(s) for s in lst])\n",
    "\n",
    "example_doc = dataset[0]['text'] + dataset[1]['text']\n",
    "example_doc = example_doc.replace(\"\\n\", \" \")\n",
    "example_doc = example_doc.replace(\"Reuter\", \" \")\n",
    "\n",
    "print(f\"The first two documents have {len(example_doc)} charaters\")\n",
    "\n",
    "example_sentence = example_doc.split(\". \")\n",
    "print(f\"We can split it into {len(example_sentence)} sentences, with an average of {np.round(avg_str_len(example_sentence), 2)} characters\")\n",
    "\n",
    "example_paragraph = example_doc.split(\"     \")\n",
    "paragraph_length = len(example_paragraph)\n",
    "print(f\"Or we can split it into {paragraph_length} paragraphs, with an average of {np.round(avg_str_len(example_paragraph), 2)} charaters\")\n",
    "\n",
    "example_chunk = [example_paragraph[i] + example_paragraph[i+1] + example_paragraph[i+2] for i in range(0, paragraph_length-2,3)]\n",
    "print(f\"Finally, we take {len(example_chunk)} chunks of two paragraphs each, averaging about {np.round(avg_str_len(example_chunk), 2)} characters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For example, from the first document we can get these different sections depenging on the split: \n",
      "One sentence of 95 characters: \n",
      "Again it seems that cocoa delivered earlier on consignment was included in the arrivals figures\n",
      "One paragraph of 261 characters: \n",
      "Arrivals for the week ended February 22 were 155,221 bags of 60 kilos making a cumulative total for the season of 5.93 mln against 5.81 at the same stage last year. Again it seems that cocoa delivered earlier on consignment was included in the arrivals figures.\n",
      "One larger chunk of 573 characters: \n",
      "Showers continued throughout the week in the Bahia cocoa zone, alleviating the drought since early January and improving prospects for the coming temporao, although normal humidity levels have not been restored, Comissaria Smith said in its weekly review.The dry period means the temporao will be late this year.Arrivals for the week ended February 22 were 155,221 bags of 60 kilos making a cumulative total for the season of 5.93 mln against 5.81 at the same stage last year. Again it seems that cocoa delivered earlier on consignment was included in the arrivals figures.\n"
     ]
    }
   ],
   "source": [
    "print(\"For example, from the first document we can get these different sections depenging on the split: \")\n",
    "print(f\"One sentence of {len(example_sentence[3])} characters: \\n{example_sentence[3]}\")\n",
    "print(f\"One paragraph of {len(example_paragraph[2])} characters: \\n{example_paragraph[2]}\")\n",
    "print(f\"One larger chunk of {len(example_chunk[0])} characters: \\n{example_chunk[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The indexes within the different lists can quickly get hard to track. For example we need to be able to tell Sentence 3 was in paragraph 2 of the first chunk in document document 0.\n",
    "We would need a system to keep track of the different hierarchical IDs, so when we run a search we can provide the necessary context with different granularities. We can visualise this ID hierarchy in a simplified example like this:\n",
    "\n",
    "\n",
    "Doc 0, Sen 0, Par 0, Chunk 0\n",
    "Doc 0, Sen 1, Par 0, Chunk 0\n",
    "\n",
    "Doc 0, Sen 2, Par 1, Chunk 0\n",
    "Doc 0, Sen 3, Par 1, Chunk 0\n",
    "Doc 0, Sen 4, Par 1, Chunk 0\n",
    "\n",
    "\n",
    "Doc 0, Sen 5, Par 2, Chunk 1\n",
    "Doc 0, Sen 6, Par 2, Chunk 1\n",
    "\n",
    "Doc 0, Sen 7, Par 3, Chunk 1\n",
    "Doc 0, Sen 9, Par 3, Chunk 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "page_content='Showers continued throughout the week in the Bahia cocoa zone, alleviating the drought since early January and improving prospects for the coming'\n",
      "page_content='Showers continued throughout the week in the Bahia cocoa zone, alleviating the drought since early January and improving prospects for the coming temporao, although normal humidity levels have not been restored, Comissaria Smith said in its weekly review.'\n",
      "page_content='Showers continued throughout the week in the Bahia cocoa zone, alleviating the drought since early January and improving prospects for the coming temporao, although normal humidity levels have not been restored, Comissaria Smith said in its weekly review.     The dry period means the temporao will be late this year.     Arrivals for the week ended February 22 were 155,221 bags of 60 kilos making a cumulative total for the season of 5.93 mln against 5.81 at the same stage last year. Again it seems that cocoa delivered earlier on consignment was included in the arrivals figures.'\n"
     ]
    }
   ],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "def split_by_chunk(documents, chunk, metadata):\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=chunk,\n",
    "        chunk_overlap=chunk/5,\n",
    "        length_function=len,\n",
    "        is_separator_regex=False,\n",
    "        keep_separator=False,\n",
    "        separators = [\"     \", \". \", \" \", \"\"]\n",
    "    )\n",
    "\n",
    "    texts = text_splitter.create_documents(documents, metadatas=metadata)\n",
    "    return texts\n",
    "\n",
    "texts = split_by_chunk([example_doc], 150, None)\n",
    "print(texts[0])\n",
    "\n",
    "texts = split_by_chunk([example_doc], 300, None)\n",
    "print(texts[0])\n",
    "\n",
    "texts = split_by_chunk([example_doc], 600, None)\n",
    "print(texts[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install --upgrade --quiet  elasticsearch langchain-openai tiktoken langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from getpass import getpass  # For securely getting user input\n",
    "model_id = \"sentence-transformers__msmarco-minilm-l-12-v3\"\n",
    "\n",
    "ELASTIC_CLOUD_ID = getpass(\"Elastic Cloud ID: \")\n",
    "ELASTIC_API_KEY = getpass(\"Elastic API Key: \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.vectorstores.elasticsearch import ElasticsearchStore\n",
    "index_name = \"reuters\"\n",
    "db = ElasticsearchStore(\n",
    "        es_cloud_id=ELASTIC_CLOUD_ID,\n",
    "        es_api_key=ELASTIC_API_KEY,\n",
    "        index_name=index_name,\n",
    "        query_field=\"text_field\",\n",
    "        vector_query_field=\"vector_query_field.predicted_value\",\n",
    "        strategy=ElasticsearchStore.ApproxRetrievalStrategy(\n",
    "                query_model_id=model_id\n",
    "    ),)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ObjectApiResponse({'acknowledged': True})"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "db.client.ingest.put_pipeline(\n",
    "    id=\"embeddings\",\n",
    "    processors=[\n",
    "        {\n",
    "            \"inference\": {\n",
    "                \"model_id\": model_id,\n",
    "                \"field_map\": {\"_ingest._value.page_content\": \"text_field\"},\n",
    "                \"target_field\": \"vector_query_field\",\n",
    "            }\n",
    "        }\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ObjectApiResponse({'acknowledged': True, 'shards_acknowledged': True, 'index': 'reuters'})"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "db.client.indices.create(\n",
    "    index=index_name,\n",
    "    mappings={\n",
    "        \"dynamic\": \"true\",\n",
    "        \"properties\": {\n",
    "            \"vector_query_field\": {\n",
    "                \"properties\": {\n",
    "                    \"predicted_value\": {\n",
    "                        \"type\": \"dense_vector\",\n",
    "                        \"dims\": 384,\n",
    "                        \"index\": True,\n",
    "                        \"similarity\": \"cosine\",\n",
    "                    }\n",
    "                }\n",
    "            },\n",
    "            \"name\" :{\"type\": \"text\"}, \n",
    "        }\n",
    "    },\n",
    "    settings={\"index\": {\"default_pipeline\": \"embeddings\"}},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset[0:1000]\n",
    "metadata = []\n",
    "content = []\n",
    "chunk_size = 500\n",
    "\n",
    "for doc in dataset:\n",
    "  content.append(doc[\"text\"])\n",
    "  metadata.append({\n",
    "      \"name\": doc[\"title\"]\n",
    "  })\n",
    "\n",
    "docs = split_by_chunk(documents=content, chunk=chunk_size, metadata = metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langchain_community.vectorstores.elasticsearch.ElasticsearchStore at 0x2ac607a10>"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "db.from_documents(\n",
    "    docs,\n",
    "    es_cloud_id=ELASTIC_CLOUD_ID,\n",
    "    es_api_key=ELASTIC_API_KEY,\n",
    "    index_name=index_name,\n",
    "    query_field=\"text_field\",\n",
    "    vector_query_field=\"vector_query_field.predicted_value\",\n",
    "    strategy=ElasticsearchStore.ApproxRetrievalStrategy(\n",
    "        query_model_id=model_id\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Article name: FRANCE FACES PRESSUE TO CHANGE POLICIES: \n",
      " top priority.\n",
      " But with unemployment nearing 11 per cent last month, and\n",
      "still rising, government supporters and some economic analysts\n",
      "said they were \n",
      "\n",
      "Article name: SWISS ECONOMY IN EXCELLENT CONDITION, OECD SAYS: \n",
      " But it said job creation should continue to absorb a modest\n",
      "increase in the workforce, leaving the unemployment rate\n",
      "unchanged at around one pct, the \n",
      "\n"
     ]
    }
   ],
   "source": [
    "response = db.similarity_search(\"rising unemployment concerns\", k=2)\n",
    "for doc in response:\n",
    "    print (f'Article name: {doc.metadata[\"name\"]}: \\n {doc.page_content} \\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#In one go:\n",
    "\n",
    "model_id = \"sentence-transformers__msmarco-minilm-l-12-v3\"\n",
    "\n",
    "ELASTIC_CLOUD_ID = getpass(\"Elastic Cloud ID: \")\n",
    "ELASTIC_API_KEY = getpass(\"Elastic API Key: \")\n",
    "\n",
    "dataset = load_dataset(\"reuters21578\", 'ModHayes', split=\"train[:1%]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_index_with_chunk(db, index_name, chunk_size, chunk_overlap_part):\n",
    "    db.client.ingest.put_pipeline(\n",
    "        id=\"embeddings\",\n",
    "        processors=[\n",
    "            {\n",
    "                \"inference\": {\n",
    "                    \"model_id\": model_id,\n",
    "                    \"field_map\": {\"_ingest._value.page_content\": \"text_field\"},\n",
    "                    \"target_field\": \"vector_query_field\",\n",
    "                }\n",
    "            }\n",
    "        ],\n",
    "    )\n",
    "\n",
    "    db.client.indices.create(\n",
    "        index=index_name,\n",
    "        mappings={\n",
    "            \"dynamic\": \"true\",\n",
    "            \"properties\": {\n",
    "                \"vector_query_field\": {\n",
    "                    \"properties\": {\n",
    "                        \"predicted_value\": {\n",
    "                            \"type\": \"dense_vector\",\n",
    "                            \"dims\": 384,\n",
    "                            \"index\": True,\n",
    "                            \"similarity\": \"cosine\",\n",
    "                        }\n",
    "                    }\n",
    "                },\n",
    "                \"name\" :{\"type\": \"text\"}, \n",
    "            }\n",
    "        },\n",
    "        settings={\"index\": {\"default_pipeline\": \"embeddings\", \"refresh_interval\" : \"1000s\"}},\n",
    "    )\n",
    "\n",
    "    metadata = []\n",
    "    content = []\n",
    "\n",
    "    for doc in dataset:\n",
    "        content.append(doc[\"text\"])\n",
    "        metadata.append({\n",
    "            \"name\": doc[\"title\"]\n",
    "        })\n",
    "\n",
    "    #docs = split_by_chunk(documents=content, chunk=chunk_size, metadata = metadata)\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=chunk_size,\n",
    "        chunk_overlap=chunk_size/chunk_overlap_part,\n",
    "        length_function=len,\n",
    "        is_separator_regex=False,\n",
    "        keep_separator=False,\n",
    "        separators = [\"     \", \". \", \" \", \"\"]\n",
    "    )\n",
    "\n",
    "    docs = text_splitter.create_documents(content, metadata)\n",
    "\n",
    "    db.from_documents(\n",
    "        docs,\n",
    "        es_cloud_id=ELASTIC_CLOUD_ID,\n",
    "        es_api_key=ELASTIC_API_KEY,\n",
    "        index_name=index_name,\n",
    "        query_field=\"text_field\",\n",
    "        vector_query_field=\"vector_query_field.predicted_value\",\n",
    "        strategy=ElasticsearchStore.ApproxRetrievalStrategy(\n",
    "            query_model_id=model_id\n",
    "        ),\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunk_sizes = [600, 300, 150]\n",
    "chunk_overlap_parts = [2, 5, 10]\n",
    "\n",
    "list = [\"how is grain production affected by weather\", \n",
    "        \"countries performing well in the stock market\",\n",
    "        \"projection for crude oil demand\", \n",
    "        \"rising unemployment concerns\"]\n",
    "\n",
    "results = {}\n",
    "\n",
    "for chunk in chunk_sizes:\n",
    "    chunk_responses = {}\n",
    "    for chunk_overlap_part in chunk_overlap_parts:\n",
    "        overlap_reponse = []\n",
    "        \n",
    "        index_name = \"reuters_\" + str(chunk) + \"_\" + str(chunk_overlap_part)\n",
    "\n",
    "        db = ElasticsearchStore(\n",
    "            es_cloud_id=ELASTIC_CLOUD_ID,\n",
    "            es_api_key=ELASTIC_API_KEY,\n",
    "            index_name=index_name,\n",
    "            query_field=\"text_field\",\n",
    "            vector_query_field=\"vector_query_field.predicted_value\",\n",
    "            strategy=ElasticsearchStore.ApproxRetrievalStrategy(\n",
    "                    query_model_id=model_id\n",
    "        ),\n",
    "        )\n",
    "\n",
    "        create_index_with_chunk(db, index_name, chunk, chunk_overlap_part)\n",
    "\n",
    "        for query in list:\n",
    "            query_response = {}\n",
    "            response = db.similarity_search(query, k=8)\n",
    "            query_response[\"query\"] = query\n",
    "            answers = []\n",
    "            for doc in response:\n",
    "                answers.append({'name' : doc.metadata[\"name\"], 'content': doc.page_content})\n",
    "            \n",
    "            query_response[\"answers\"] = answers\n",
    "            overlap_reponse.append(query_response)\n",
    "        \n",
    "        chunk_responses[chunk_overlap_part] = overlap_reponse\n",
    "    results[chunk] = chunk_responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open('result.json', 'w') as fp:\n",
    "    json.dump(results, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "U.S. DATA POINT TO CAPITAL SPENDING SLOWDOWN\n",
      "U.S. DATA POINT TO CAPITAL SPENDING SLOWDOWN\n",
      "U.S. DATA POINT TO CAPITAL SPENDING SLOWDOWN\n",
      "U.S. DATA POINT TO CAPITAL SPENDING SLOWDOWN\n",
      "U.S. DATA POINT TO CAPITAL SPENDING SLOWDOWN\n",
      "U.S. DATA POINT TO CAPITAL SPENDING SLOWDOWN\n",
      "BAHIA COCOA REVIEW\n",
      "SWISS ECONOMY IN EXCELLENT CONDITION, OECD SAYS\n",
      "OPEC MAY HAVE TO MEET TO FIRM PRICES - ANALYSTS\n"
     ]
    }
   ],
   "source": [
    "print(results[600][2][1][\"answers\"][0][\"name\"])\n",
    "print(results[600][5][1][\"answers\"][0][\"name\"])\n",
    "print(results[600][10][1][\"answers\"][0][\"name\"])\n",
    "print(results[300][2][1][\"answers\"][0][\"name\"])\n",
    "print(results[300][5][1][\"answers\"][0][\"name\"])\n",
    "print(results[300][10][1][\"answers\"][0][\"name\"])\n",
    "print(results[150][2][1][\"answers\"][0][\"name\"])\n",
    "print(results[150][5][1][\"answers\"][0][\"name\"])\n",
    "print(results[150][10][1][\"answers\"][0][\"name\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "U.S. GRAIN CARLOADINGS FALL IN WEEK\n",
      "U.S. GRAIN CARLOADINGS FALL IN WEEK\n",
      "U.S. GRAIN CARLOADINGS FALL IN WEEK\n",
      "U.S. GRAIN CARLOADINGS FALL IN WEEK\n",
      "U.S. GRAIN CARLOADINGS FALL IN WEEK\n",
      "U.S. GRAIN CARLOADINGS FALL IN WEEK\n",
      "U.S. GRAIN CARLOADINGS FALL IN WEEK\n",
      "U.S. GRAIN CARLOADINGS FALL IN WEEK\n",
      "U.S. GRAIN CARLOADINGS FALL IN WEEK\n"
     ]
    }
   ],
   "source": [
    "print(results[600][2][0][\"answers\"][0][\"name\"])\n",
    "print(results[600][5][0][\"answers\"][0][\"name\"])\n",
    "print(results[600][10][0][\"answers\"][0][\"name\"])\n",
    "print(results[300][2][0][\"answers\"][0][\"name\"])\n",
    "print(results[300][5][0][\"answers\"][0][\"name\"])\n",
    "print(results[300][10][0][\"answers\"][0][\"name\"])\n",
    "print(results[150][2][0][\"answers\"][0][\"name\"])\n",
    "print(results[150][5][0][\"answers\"][0][\"name\"])\n",
    "print(results[150][10][0][\"answers\"][0][\"name\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
